{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDVZyDzq0c4N",
        "outputId": "ee659e1c-eda6-481c-80c8-c8d0cba3e55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/664.8 MB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Cài đặt các thư viện cần thiết\n",
        "!pip install transformers torch scikit-learn pandas\n",
        "\n",
        "# Nhập các thư viện\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải dữ liệu từ file CSV\n",
        "data = pd.read_csv('/content/VI_IMDB.csv')\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(data.head())\n",
        "\n",
        "# Ánh xạ nhãn cảm xúc thành số (positive: 1, negative: 0)\n",
        "label_map = {\"positive\": 1, \"negative\": 0}\n",
        "data['label'] = data['sentiment'].map(label_map)\n",
        "\n",
        "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['vi_review'].tolist(),\n",
        "    data['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Kiểm tra kích thước tập dữ liệu\n",
        "print(f\"Tập huấn luyện: {len(train_texts)} mẫu\")\n",
        "print(f\"Tập kiểm tra: {len(val_texts)} mẫu\")"
      ],
      "metadata": {
        "id": "XlHzogZo0jng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải tokenizer của PhoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# Token hóa tập huấn luyện và kiểm tra\n",
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "val_encodings = tokenizer(\n",
        "    val_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Tạo class Dataset tùy chỉnh\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Tạo tập dữ liệu huấn luyện và kiểm tra\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "val_dataset = SentimentDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "bU0pouEn0nGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải mô hình PhoBERT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)\n",
        "\n",
        "# Thiết lập tham số huấn luyện\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",  # Thay evaluation_strategy thành eval_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Khởi tạo Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1gGvKUSO0sv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đánh giá mô hình trên tập kiểm tra\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Kết quả đánh giá:\", eval_results)\n",
        "\n",
        "# Tính toán các chỉ số bổ sung (accuracy, precision, recall, F1)\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Dự đoán trên tập kiểm tra\n",
        "predictions = trainer.predict(val_dataset)\n",
        "pred_labels = predictions.predictions.argmax(-1)\n",
        "\n",
        "# Tính các chỉ số\n",
        "accuracy = accuracy_score(val_labels, pred_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(val_labels, pred_labels, average='binary')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "uThg6vsw0xfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lưu mô hình và tokenizer\n",
        "trainer.save_model(\"./phobert-sentiment-model\")\n",
        "tokenizer.save_pretrained(\"./phobert-sentiment-model\")\n",
        "\n",
        "# Tải mô hình để dự đoán\n",
        "from transformers import pipeline\n",
        "\n",
        "# Tạo pipeline dự đoán\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"./phobert-sentiment-model\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Thử dự đoán trên một đánh giá mới\n",
        "new_review = \"Bộ phim này thật sự rất tệ và thiếu sức sống!\"\n",
        "result = sentiment_pipeline(new_review)\n",
        "print(\"Dự đoán:\", result)\n",
        "\n",
        "# Chuyển đổi nhãn dự đoán\n",
        "label_map_reverse = {0: \"negative\", 1: \"positive\"}\n",
        "predicted_label = label_map_reverse[int(result[0]['label'].split('_')[-1])]\n",
        "print(f\"Cảm xúc dự đoán: {predicted_label}\")"
      ],
      "metadata": {
        "id": "xGB1CwPd01a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Assuming you have `dataset` (a Hugging Face DatasetDict) and a tokenizer\n",
        "X = dataset['train']['text']\n",
        "y = dataset['train']['label']\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Subset the dataset\n",
        "    train_subset = dataset['train'].select(train_idx.tolist())\n",
        "    val_subset = dataset['train'].select(val_idx.tolist())\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized_train = train_subset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
        "    tokenized_val = val_subset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)\n",
        "\n",
        "    # Define Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_fold{fold}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,  # Define accuracy/F1 metric separately\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "RT46bomSr-Vb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}